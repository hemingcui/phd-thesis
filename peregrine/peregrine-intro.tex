The last chapter presents \tern, the first multithreading system that is 
efficient, stable, and deterministic. However, \tern enforces 
best-effort determinism, and its executions may deviate from the memoized 
schedules when data races exist. Thus, a second challenge on building \smt 
arises: how to efficiently enforce schedules without deviation? This challenge 
also exists in the area of deterministic execution and replay for decades. To 
address this challenge, this chapter presents \peregrine, our second \smt (and 
also \dmt) system with a new technique called \emph{schedule relaxation}.

\section{Introduction} \label{sec:peregrine-intro}

%% Why multithreading is so difficult to get right. Too many schedules.
%% All inputs, each input (define nondeterminism). Must refer to Chapter 2:
%% motivation.
As described in Chapter~\ref{sec:smt-motivation}, a root cause that makes 
multithreaded programs so difficult to get right is: these
programs have exponentially many possible schedules for all inputs at
runtime. Even running on the same input, the concurrently running threads of a
program may interleave in too many different ways, depending on such factors as
hardware timing and OS scheduling. This is the so called ``nondeterminism."
Considering all inputs, the number of possible schedules is even greater. It is
extremely challenging to understand, test, analyze, or verify all
these schedules in a multithreaded program. Therefore, a concurrency bug within
an unchecked schedule can show up in production runs and lead to severe
failures and vulnerabilities.

To reduce the number of possible schedules and make multithreading easier to get
right, two complementary techniques have been invented by researchers recently.
\smt~\cite{determinator:osdi10, cui:tern:osdi10}, which is created by my 
collaborators and me, aims to reduce the number possibles schedules on all 
inputs. \dmt~\cite{dmp:asplos09,kendo:asplos09,coredet:asplos10,
dos:osdi10,grace:oopsla09} addresses the nondeterminism problem, and it focuses
on reducing the number of possible schedules on each input down to one. Notably,
our \tern system described in Chapter~\ref{sec:tern} is the first 
multithreading system that is both stable and deterministic. Unfortunately, 
despite these effort, an open challenge~\cite{wodet11}
well recognized by the research community remains: how to efficiently enforce 
schedules without deviation for general multithreaded programs on
commodity multiprocessors. When running with existing \dmt systems, programs' 
executions either incur prohibitive overhead, or may show deviate behaviors if 
there are data races (\ie, these executions are not \emph{fully deterministic}).

Specifically, existing DMT systems enforce two forms of schedules: (1) a
\emph{mem-schedule} is a deterministic schedule of shared memory
accesses~\cite{dmp:asplos09,coredet:asplos10,dos:osdi10}, such as
\vv{load/store} instructions, and (2) a \emph{sync-schedule} is a
deterministic order of synchronization
operations~\cite{kendo:asplos09,cui:tern:osdi10}, such as
\vv{lock()/unlock()}.  Enforcing a mem-schedule is fully deterministic even
for programs with data races, but may incur prohibitive overhead (\eg,
roughly 1.2\X-6\X~\cite{coredet:asplos10}).  Enforcing a sync-schedule is
efficient (\eg, average 16\% slowdown~\cite{kendo:asplos09}) because
most code does not control synchronization and can still run in
parallel, but a sync-schedule is only deterministic for race-free
programs, when, in fact, most real programs have races, harmful or
benign~\cite{lu:concurrency-bugs,syncfinder:osdi10}.  The dilemma is,
then, to pick either fully determinism or efficiency, but not both.

Our key insight is that although most programs have races, these races
tend to occur only within minor portions of an execution, and the majority
of the execution is still race-free.  Thus, we can resort to a
mem-schedule only for the ``racy'' portions of an execution and enforce a
sync-schedule otherwise, combining both the efficiency of sync-schedules
and the determinism of mem-schedules. We call these combined schedules
\emph{hybrid schedules}.

Based on this insight, we have built \peregrine, an efficient \dmt
system to address the aforementioned open challenge.
When a program first runs on an input, \peregrine records a detailed execution
trace including memory accesses in case the execution runs into races.
\peregrine then \emph{relaxes} this detailed trace into a hybrid schedule,
including (1) a total order of synchronization operations and (2) a set of
execution order constraints to deterministically resolve each occurred race. 
When the same input is provided again, \peregrine can reuse this schedule
deterministically and efficiently.

Reusing a schedule only when the program input matches exactly is too
limiting, and we aim to make \peregrine also a \smt system that can frequently
reuse schedules on a wide range of inputs.  Fortunately, the schedules
\peregrine computes are often
``coarse-grained" and reusable on a broad range of inputs.  Indeed, our
previous work has shown that a small number of sync-schedules can often cover
over 90\% of the workloads for real programs such as
\apache~\cite{cui:tern:osdi10}.  The higher the reuse rates, the more efficient
\peregrine is.

% Moreover, by reusing schedules, \peregrine makes program
% behaviors more \emph{stable} across different inputs, so that slight input
% changes do not lead to vastly different schedules~\cite{cui:tern:osdi10} and
% thus ``\emph{input-heisenbugs}" where slight input changes cause concurrency
% bugs to appear or disappear.

Before reusing a schedule on an input, \peregrine must check that the input
satisfies the input constraints of the schedule, so
that (1) the schedule is feasible, \ie, the execution on the input will reach
all events in the same deterministic order as in the schedule and (2) the
execution will not introduce new races. (New races may occur if they are
\emph{input-dependent}; see \S\ref{sec:peregrine-interthread-slice}.)  A na\"ive
approach is to collect preconditions from all input-dependent branches in
an execution trace.  For instance, if a branch instruction inspects
input variable \vv{X} and goes down the true branch, we collect a
precondition that \vv{X} must be nonzero.  Preconditions collected via
this approach ensures that an execution on an input satisfying the
preconditions will always follow the path of the recorded execution in all
threads.  However, many of these branches concern thread-local
computations and do not affect the program's ability to follow the
schedule. Including them in the preconditions thus unnecessarily decreases
schedule-reuse rates.

How can \peregrine compute sufficient preconditions to avoid new races and
ensure that a schedule is feasible?  How can \peregrine filter out unnecessary
branches to increase schedule-reuse rates?  Our previous
work, \tern~\cite{cui:tern:osdi10}, requires developers to grovel through the
code and mark the input affecting schedules; even so, it does not
guarantee full determinism if there are data races.

\peregrine addresses these challenges with two new
program analysis techniques.  First, given an execution trace and a hybrid
schedule, it computes sufficient preconditions using
\emph{determinism-preserving slicing}, a new precondition
slicing~\cite{castro:bouncer} technique designed for multithreaded
programs.  Precondition slicing takes an execution trace and a
\emph{target} instruction in the trace, and computes 
a \emph{trace slice} that captures the instructions required for the
execution to reach the target with equivalent operand
values.  Intuitively, these instructions include ``branches whose
  outcome matters" to reach the target and ``mutations that affect the
  outcome of those branches"~\cite{castro:bouncer}.  This trace slice
typically has much fewer branches than the original execution trace,
so that we can compute more relaxed preconditions.  However, previous
work~\cite{castro:bouncer} does not compute correct trace slices for
multithreaded programs or handle multiple targets; our slicing
technique correctly handles both cases.

Our slicing technique often needs to determine whether two pointer
variables may point to the same object.  \emph{Alias
  analysis} is the standard static technique to answer these queries.
Unfortunately, one of the best alias analyses~\cite{bddalias:pldi04} yields
overly
imprecise results for 30\% of the evaluated programs, forcing \peregrine to
reuse
schedules only when the input matches almost exactly.  The reason is that
standard
alias analysis has to be conservative and assume all possible executions,
yet \peregrine cares about alias results
according only to the executions that reuse a specific schedule.  To
improve precision, \peregrine uses \emph{schedule-guided simplification} to
first simplify a program according to a schedule, then runs standard alias
analysis on the simplified program to get more precise results.  For
instance, if the schedule dictates eight threads, \peregrine can clone
the corresponding thread function eight times, so that alias analysis can
separate the results for each thread, instead of imprecisely merging
results for all threads.

We have built a prototype of \peregrine that runs in user-space.
It automatically tracks \vv{main()}
arguments, data read from files and sockets, and values
returned by \vv{random()}-variants as input.  It handles long-running servers by
splitting their executions into \emph{windows} and
reusing schedules across windows~\cite{cui:tern:osdi10}.
The hybrid schedules it computes are fully deterministic for programs that
(1) have no nondeterminism sources beyond thread scheduling, data races, and
inputs tracked by \peregrine and (2) adhere to the assumptions of the tools
\peregrine uses.

We evaluated \peregrine on a diverse set of \peregrinenprog programs, including
the
\apache web server~\cite{apache}; three desktop programs, such as
\pbzip~\cite{pbzip2}, a parallel compression utility; implementations of
12 computation-intensive algorithms in the popular \splash and \parsec
benchmark suites; and \racey~\cite{racy-stress}, a benchmark with numerous
intentional races for evaluating deterministic execution and replay
systems.  Our results show that \peregrine is both deterministic and efficient
(executions reusing schedules range from 68.7\% faster to 46.6\% slower
than nondeterministic executions); it can frequently reuse schedules for
half of the programs (\eg, two schedules cover all possible inputs to
\pbzip compression as long as the number of threads is the same); both its
slicing and
simplification techniques are crucial for increasing schedule-reuse rates,
and have reasonable overhead when run offline; its recording overhead
is relatively high, but can be reduced using existing
techniques~\cite{idna:vee06}; and it requires no manual effort except a
few annotations for handling server programs and for improving precision.

Our main contributions are the schedule-relaxation approach and \peregrine, an
 efficient stable and deterministic multithreading system.  Additional 
contributions include
the ideas of hybrid schedules, determinism-preserving slicing, and
schedule-guided simplification.  To our knowledge, our slicing technique
is the first to compute correct (non-trivial) preconditions for
multithreaded programs.
We believe these ideas apply beyond \peregrine (\S\ref{sec:peregrine-deploy}).

The remainder of this chapter is organized as follows.  We first present a
high-level design of \peregrine (\S\ref{sec:peregrine-design}).  We then
describe its
core ideas: hybrid schedules (\S\ref{sec:peregrine-schedule}),
determinism-preserving slicing (\S\ref{sec:peregrine-slice}), and
schedule-guided
simplification (\S\ref{sec:peregrine-guide}).  We then present implementation
issues
(\S\ref{sec:peregrine-impl}) and evaluation (\S\ref{sec:peregrine-eval}).  We
finally discuss
related work (\S\ref{sec:peregrine-related}) and conclude
(\S\ref{sec:peregrine-summary}).
