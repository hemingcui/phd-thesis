\chapter{Introduction} \label{sec:intro}

%% P1: Multi-threading is critical and popular.
Multithreading has become pervasive and critical because of two computing
trends. First, due to the physical constraints on circuit speed, computing
platforms are having more and more cores rather than faster and faster
single-core. In order to harness the power of multi-core, developers write more
and more multithreaded programs on these platforms. Second, the emerging cloud
computing trend requires networking services (\eg, \http servers and database
servers) to process more and more requests concurrently, which also pushes
developers to write more and more multithreaded programs. These two trends will
continue and multithreading will become increasingly pervasive and critical.

%% P2: However, it is hard to get right.
Unfortunately, despite decades of effort from both academia and industry,
multithreaded programs are still notoriously difficult to get right, and these
programs are plagued with harmful concurrency bugs that can cause wrong outputs,
program crashes, security breaches, and so on. Our research reveals that a key
reason of this difficulty is: multithreaded programs have too many
possible thread interleavings (or \emph{schedules}) at runtime. Even running on 
the same input, the concurrently running threads of a program may interleave in 
too many different ways, depending on such factors as hardware timings and OS
scheduling. Considering all inputs, the number of possible schedules is even
greater. Chapter~\ref{sec:smt-motivation} will quantify the number of
possible schedules in multithreaded programs running in a traditional
multithreading runtime. It is extremely challenging to understand, test, 
analyze, or verify all these schedules in a multithreaded program and 
make sure that they are free of concurrency bugs. Therefore, a concurrency bug 
within an unchecked schedule can show up in production runs and lead to severe 
failures and vulnerabilities.

%% P3: DMT: one possible approach. And its limitations.
To make multithreading easier to get right, researchers have proposed an idea 
called \emph{Deterministic Multithreading} (or \dmt)~\cite{dthreads:sosp11, 
dpj:oopsla09, dmp:asplos09, kendo:asplos09, coredet:asplos10} that always 
enforces the same schedule on the same input, greatly improving reliability for 
a multithreaded program on each input. However, as we will further analyze in
Chapter~\ref{sec:smt-motivation}, although \dmt is useful, it is not as useful
as commonly perceived. The reason is because a typical \dmt system can enforce
very different schedules on slightly different inputs, artificially reducing
programs' robustness on input perturbations, and the number of possible
schedules on all inputs are still enormous. Therefore, multithreaded programs
are still very hard to understand, test, analyze, or verify.

%% P: StableMT: our new approach and thesis.
In order to reduce the number of schedules for all inputs, we have studied the
relation between inputs and schedules of real-world programs, and made an
interesting discovery: many programs need only a small set of schedules to
efficiently process a wide range of inputs~\cite{smt:cacm}! Leveraging this
discovery, we have invented a new idea called \emph{Stable Multithreading} (or 
\smt) that reuses each schedule on a wide range of inputs. By reusing a 
schedule on as many as inputs, \smt stabilizes program behaviors against small 
input perturbations and greatly reduces the number of possible schedules for all
inputs. By addressing the key reason that makes multithreading difficult to get
right, StableMT greatly simplifies understanding, testing, analyzing,
and verification of multithreaded programs. Actually, \smt is
complementary to \dmt, and a system can be both stable and deterministic. To 
fully understand the potential of \smt, Chapter~\ref{sec:smt-motivation} will 
discuss in details on the three types of multithreading runtimes: the 
traditional multithreading, the deterministic multithreading (\dmt), and the 
stable multithreading (\smt).


%% P: intro to the systems we built: with each addressing different challenges.
To make \smt real, I have worked with Columbia and CMU researchers to build
three \smt (and also \dmt) systems, \tern~\cite{cui:tern:osdi10},
\peregrine~\cite{peregrine:sosp11}, and \parrot~\cite{parrot:sosp13}, with each
addressing a distinct research challenge. We identify and address these three
challenges as follows.

\para{Challenge 1: How to compute highly reusable schedules for different
inputs?} The more reusable a schedule is, the fewer schedules are needed.
However, finding highly reusable schedules is hard with existing static or
dynamic techniques, because statically computed schedules are not guaranteed to
work at runtime due to the halting problem, and dynamically computing schedules
may cause prohibitive overhead.

To address this challenge, our first \smt system, \tern~\cite{cui:tern:osdi10}
(Chapter~\ref{sec:tern}), proposes a technique called \emph{schedule
memoizatoin} that memoizes a set of past, working schedules, and then reuses
these schedules on future inputs when possible. This technique is inspired by a
real-world analogy that human and animals tend to migrate along past, familiar
routes and avoid possible hazards in unknown ones. In order to find a schedule
suitable for an input, \tern leverages a set of advanced program analysis
techniques to compute preconditions on inputs that match a schedule. Evaluation
on a diverse set of popular programs shows that \tern can reuse a small set of
schedules to process a wide range of inputs. For instance, just 100 schedules
for the \apache web server can process 90.3\% of a 4-day trace (122K requests)
from the Columbia CS website.

\para{Challenge 2: How to efficiently make executions follow schedules and do
not deviate?} This challenge also exists in the area of deterministic
execution and replay for decades. Existing work typically enforces two types of
schedules: a total order of shared memory accesses (for short, \memsched), and a
total order of synchronization operations (for short, \syncsched). The
\memscheds are fully deterministic even with data races, but they are several
times slower than traditional multithreading. The \syncscheds incur only modest
overhead because most code is not synchronization and thus can still run in
parallel, but these schedules may deviate if there are data races. Overall,
despite much research effort, people can only choose either full determinism or
efficiency, but not both.

To address this challenge, our second \smt system,
\peregrine~\cite{peregrine:sosp11} (Chapter~\ref{sec:peregrine}), stands out
with an observation: although many programs have races, the races tend to occur
only within minor portions of an execution, and the majority of the execution is
still race-free. Therefore, we can enforce a \syncscheds in the race-free
portions of an execution and resort to a \memsched only in the racy portions,
combining both the efficiency of sync-schedules and determinism of \memscheds. 
\peregrine implements this form of hybrid-schedule with a new technique called
\emph{schedule ralaxation}: it first records an execution trace of
all executed instructions on a new input, and then relaxes the trace into a
highly reusable hybrid-schedule. Evaluation on a diverse set of programs shows
that \peregrine is deterministic and efficient, and can frequently reuse
schedules for half of the evaluated programs. \peregrine has been featured in
sites such as \acmtechnews, \tgdaily, and \physorg.

\para{Challenge 3: How to make \smt simple, fast, and deployable?} In the last
five years, \smt has achieved promising advances and attracted the research
community's interest. Numerous notable \smt systems~\cite{determinator:osdi10,
cui:tern:osdi10, peregrine:sosp11, dthreads:sosp11, ics:oopsla13} have been
built, including our \tern and \peregrine systems. However, it remains an open
challenge whether \smt can be made simple, fast, and deployable. Existing \smt
systems either run into slow schedules that \emph{serialize} parallel
computation, or are fairly hard to deploy due to their high complexity (\eg,
\tern and \peregrine require sophisticated program analysis).

To address with this challenge, our third \smt system, 
\parrot~\cite{parrot:sosp13}
(Chapter~\ref{sec:parrot}), presents a simple, deployable runtime that enforces
a well-defined round-robin schedule for synchronization operations, vastly
reducing the number of schedules. To address the serialization problem in \smt,
we come up with an insight based on the famous 80-20 rule: most threads spend
most execution time in only a few core computations, and we only need to make 
these core computations parallel. Accordingly, we create a new abstraction
called \emph{performance hints} for developers to annotate core computations.
These hints, which just try to get to faster schedules that improve parallelism
of core computations, are not real synchronization, and can be safely ignored
without affecting correctness of a program. Evaluation on a wide range of 108
popular programs (\eg, \bdb and \mplayer), roughly 10$\times$ more programs than
any prior \smt or \dmt evaluation, and about 4$\times$ more programs than all
prior evaluations combined, shows that, these hints are easy to add and make
\parrot fast (merely 12.7\% mean overhead on 24-core machines). To encourage
deployment, we have made \parrot's source code, entire benchmarks, and raw
evaluation results public at: \github.

%% P5: StableMT's applications.
In addition to implementing \smt systems, we have also been applying \smt to 
improve several reliability tools and demonstrated \smt's potential. First, 
we have applied \tern~\cite{cui:tern:osdi10} and 
\peregrine~\cite{peregrine:sosp11}
respectively to reproduce several real-world concurrency bugs, and found that
\smt's schedules can consistently mask or reproduce these bugs, and these
schedules are stable to input perturbations (Chapter~\ref{sec:tern} 
and~\ref{sec:peregrine}). Second, we have applied \peregrine to greatly improve 
the precision of program
analysis~\cite{wu:pldi12} and verification~\cite{wu:pldi12}, leading to several
new harmful concurrency bugs detected in heavily-studied programs 
(Chapter~\ref{sec:peregrine}). Third, we have quantitatively shown that \parrot 
can increase the coverage by many orders
of magnitudes (Chapter~\ref{sec:parrot}) for model checking, an advanced 
technique that systematically
tests schedules and try to find bugs~\cite{parrot:sosp13, dbug:spin11,
modist:nsdi09}. Each of our three \smt systems can benefit all these three
reliability tools with modest engineering effort, because many techniques
in our \smt systems are common. For instance, \parrot can also make
reproducing concurrency bugs much easier because it enforces a similar form
of schedule to that in \tern.

These \smt applications have attracted the community's interests. For instance,
some of our \smt techniques and ideas have been leveraged by University of
Washington researchers to compute a small set of schedules to cover all or most
inputs for some multithreaded programs~\cite{bergan:oopsla13}.

The rest of the thesis is organized as follows. Chapter~\ref{sec:smt-motivation}
presents the motivation and background of \smt. Chapter~\ref{sec:tern} presents
the \tern system, and our results on applying it to make reproducing concurrency
bugs easier. Chapter~\ref{sec:peregrine} describes the \peregrine system, and
how much it can improve precision of existing program analysis techniques as
well as making reproducing concurrency bugs easier. Chapter~\ref{sec:parrot}
introduces the \parrot system, and our advances on applying it to greatly
improve coverage of model checking. Chapter~\ref{sec:conclusion} concludes.



