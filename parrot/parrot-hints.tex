\section{Performance Hint Abstractions} \label{sec:parrot-hints}

\parrot provides two performance-hint abstractions: a \emph{soft barrier} and
a \emph{performance critical section}.  This section describes these abstractions and
their usage.


\subsection{Soft Barrier} \label{sec:soft-barrier}

A \vcompute encourages the scheduler to coschedule a group of threads at
given program points.  It is for performance only, and a scheduler can
ignore it without affecting correctness.  It operates as a
barrier with deterministic timeouts in \parrot, helping \parrot switch to faster
schedules that avoid serializing parallel computations.  The interface is
\vspace{-3 mm}
\lgrindfile{parrot/code/compute.cpp}
\vspace{-1 mm}

\noindent
One thread calls \v{soba\_init(N, key, timeout)} to initialize the barrier
named \v{key}, logically indicating that a group of \v{N} threads will
be spawned.  Subsequently, any thread calling \v{soba\_wait(key)}
will block until either (1) \v{N}-1 other threads have also called
\v{soba\_wait(key)} or (2) \v{timeout} time has elapsed since the first
thread arrived at the barrier.  This timeout is made deterministic by \parrot
(\S\ref{sec:parrot-scheduler}).  \v{soba\_init} can be called multiple
times: if the number of coscheduled threads varies but is known at runtime,
the soft barrier can be initialized before each use.  Both \v{key} and
\v{timeout} in \v{soba\_init} are optional.  An absent \v{key} refers to a
unique anonymous barrier.  An absent \v{timeout} initializes the barrier
with the default timeout.

A \compute may help developers express coscheduling intent to classic
nondeterministic schedulers~\cite{coschedule}.  One advantage is that it
makes the group of threads and program points explicit.  It is more robust
to developer mistakes than a real barrier~\cite{coschedule:sigmetrics96}
for coscheduling purposes because schedulers cannot ignore a real barrier.


\subsection{Performance Critical Section} \label{sec:parrot-performance-critical-section}

A \vnondet identifies a code region as a potential bottleneck
and encourages the scheduler to get
through the region fast.  When a thread enters a performance critical
section, \parrot removes the thread from the round-robin scheduling 
and delegates it to the OS scheduler for nondeterministic execution.  \parrot thus gains speed
from allowing additional schedules.  The interface is
\vspace{-3 mm}

\lgrindfile{parrot/code/nondet.cpp}
\vspace{-1 mm}

\noindent
The \v{pcs\_enter} function marks the entry of a performance critical section and
\v{pcs\_exit} the exit.

\subsection{Usage of the Two Hints} \label{sec:parrot-hints-usage}

\para{Soft barrier.} Developers should generally use soft barriers to align
high-level, time-consuming parallel computations, such as the \v{compress}
calls in \pbzip. A generic method is to use performance debugging tools or
\parrot's logs to detect synchronizations excessively delayed by \parrot's
round-robin scheduling, then identify the serialized parallel computations.

A second method is to add soft barriers based on parallel computation
patterns.  Below we describe how to do so based on four parallel
computation patterns we observed from the \nprog evaluated programs.

\begin{enumerate}

\item[$\bullet$] {\em Data partition}. Data is partitioned among worker
  threads, and each worker thread computes on a partition. This pattern is
  the most common; \ndatapartition out of the \nprog programs follow this
  pattern, including the programs with fork-join parallelism.  Most
  programs with this pattern need no soft barriers.  In rare cases when
  soft barriers are needed, developers can add \v{soba\_wait} before each
  worker's computation.  These soft barriers often work extremely well.

\item[$\bullet$] {\em Pipeline}. The threads are split into stages of a
  pipeline, and each item in the workload flows through the pipeline
  stages.  \ferret, \dedup, \vips, and \xtwosixfour from \parsec~\cite{parsec} follow this pattern.
  These programs often need soft barriers because
  threads have different roles and thus do different synchronizations,
  causing default schedules to serialize computations.  The
  methodology is to align the most time-consuming stages of the
  pipeline.

\item[$\bullet$] {\em Map-reduce}. Programs with this pattern use both data
  partition and pipeline, so the methodology follows both: align the
  map function and, if the reduce function runs for roughly the same
  amount of time as the map function, align reduce with map.

\item[$\bullet$] {\em Workpile}. The workload consists of a pile of independent
  work items, processed by worker threads running in parallel.  Among the
  programs we evaluated, \bdb, \openldap, \redis, \pfscan, and \aget fall
  in this category.  These programs often need no soft barriers because
  it typically takes similar times to process most items.

\end{enumerate}


\para{Performance critical section.} Unlike a soft barrier, a \nondet may trade
some determinism for performance. Consequently, it should be applied with caution,
only when (1) a code region imposes high performance overhead on deterministic execution
and (2) the additional schedules have been thoroughly checked by tools or
advanced developers.  Fortunately, both conditions are often easy to meet
because the synchronizations causing high performance overhead are often low-level
synchronizations (\eg, lock operations protecting a shared counter),
straightforward to analyze with local reasoning or model checkers.

Of all \nprog evaluated programs, only \nprognondethints need performance critical
sections for reasonable performance; all other \nprognonondethints
programs need not trade determinism for performance.
Moreover, \ecosys verified all schedules in all 4 real-world programs that
need performance critical sections, providing high assurance.

Developers can identify where to add performance critical sections also
using performance debugging tools.  For instance, frequent
synchronizations with medium round-robin delays are often good candidates for
a performance critical section.  Developers can also focus on such
patterns as synchronizations in a tight loop,
synchronizations inside an abstraction boundary (\eg, \v{lock()}
inside a custom memory allocator), and tiny critical sections (\eg,
``\v{lock(); x++; unlock();}'').  

%% A dominant pattern is to mark a tight loop with synchronizations as a
%% performance critical section, so \parrot can pass a thread running in the
%% loop to the OS scheduler for quick finish.

%% \nondet hints are added to tell the \parrot runtime to ignore 
%% some sync operations in perfomance critical 
%% sections, and the per-thread computation in these programs 
%% and not roughly the same (so \compute hints are not 
%% suitable for these cases). 

%% usage: low-level

%% experience: 

%% Nondeterminism hints revert back to nondeterministic execution for
%% carefully chosen code regions.  These regions are often already ``hot''
%% during nondeterministic execution by doing many synchronizations at a high
%% rate, so they cause even very high overhead when executed
%% deterministically.  By moving threads in nondeterministic regions from the
%% round-robin scheduler to the OS's scheduler, \parrot gains speed.


%% \v{nondet(var)} marks all synchronizations on synchronization variable
%% \v{var} as nondeterministic, so \parrot excludes the synchronizations from
%% its round-robin scheduling. Frequently a code region executes many
%% synchronizations touching several synchronization variables, and
%% developers can use

%% \parrot treats \v{nondet\_begin} as a special synchronization that
%% participates in round-robin scheduling, so that a nondeterministic region
%% always begins execution deterministically.  However, \parrot does not know
%% how long the region may run, so it has to let the region end
%% nondeterministically.  After the thread calls \v{nondet\_end}, \parrot adds
%% the thread back to the round-robin scheduling.

%% \para{Usage.} Unlike \compute hints, \nondet hints may trade off 
%% an amount of determinism for performance, so it should be applied with
%% caution, only when (1) a code region has high overhead on deterministic
%% execution and (2) the additional schedules have been thoroughly checked by
%% tools or advanced developers inspecting the code region.  Fortunately,
%% both conditions are often easy to meet because the synchronizations
%% causing high overhead tend to be frequent, low-level synchronizations,
%% easy to analyze with local reasoning. Among all programs 
%% we have evaluated, four real applications \pfscan, three STL programs 
%% \partition, \nthelement, and \partialsort, and five 
%% benchmark programs \fluidanimate, \fmm, 
%% \cholesky, \raytrace from \splashx, and \ua and all fall into this category.
%% For all the four real applications, our ecosystem can 
%% effectively check all these schedules in the \nondet region 
%% within a few hours. For all the five benchmarks programs, 
%% although our ecosystem can not check all their schedules in 
%% \nondet regions within one day's run, we still considered 
%% these results promising because people normally care 
%% about the correctness of real applications over benchmark 
%% programs.

%% Heming: we do not prefer to mention this because this 
%% pattern is not that common in our non-det programs. Only 
%% pfscan and fluidanimate do this.
%% A common pattern we observed on the
%% programs we evaluated is when the program uses a tiny critical section to
%% increment or decrement a shared counter.  It easy to determine based on
%% purely local reasoning that The executions of these critical section are
%% commutable.  This pattern appears in 6 out of 9 programs with \nondet
%% hints.


