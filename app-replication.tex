\section{Leveraging \smt to Build Transparent Multithreaded State Machine Replication} \label{sec:replication}

In this section, we first introduce the background of state machine replication 
and its current limitations on building state machine replications for general 
multithreaded programs, and then present our design.

\subsection{Background} \label{sec:replication-intro}

Due to the trends of cloud computing, building higly-
available and fast applications have become critical. In order to 
provide high availability, the state machine replication (or SMR) approach has become 
popular. Todo: briefly introduce SMR use Paxos to totally order all requests. In order to
build fast services, people introduce read-only optimizations for read requests.

Unfortunately, despite much effort from academia and industry, it is still 
insufficient for software vendors to provide practical state machine 
replication service to general applications, because of two challenges. Existing 
state machine replication techniques typically require developers to replicate 
a specific subset of program states (with APIs) instead of the whole program 
states, which requires strong distributed sytems expert knowledge, and error prone 
(cite the MongoDB exmample). Second, existing SMR infrastructures assume all 
replicas always run the same schedule for the same input, pracitally restricing 
the number of running threads to be only 1 and preventing them fully leverage the 
power of the multi-core hardware.

Addressing the first challenge requires a general design of APIs for SMR. 
Todo: motivate people that we use design the interface at the socket API level.

Addressing the second challenge requires a systematic way of to ensure the same 
schedules. Todo: introduce \smt, which can greatly reduce the number of schedules for 
multithreaded programs.

This article made two major contributions towards building practical state-
machine replications for multithreaded programs. First, how to reach 
consensus on general network inputs (including input values and timings) 
across replicas. Second, how to perform read-only 
optimization for general applications, which is hard, because some 
semantically read only operations (such as get() in key-value store and GET 
in http servers) may modify states of server programs. Todo: should we 
include any other challenges that we have discussed? E.g., recovery, 
leveraging StableMT, and logical clocks.

Our system is the first system on building practical state-machine 
replications for general multi-threaded programs. Todo: how it works: reach 
consensus on general socket operations, LD\_PRELOAD, transparent (do not need 
to annotate shared states in a program) replications for servers.

Introduce the fault-tolerance and performance features (guarantees) of our 
system.

Practical highlights of our system: evaluated a good range of popular server 
programs, ranging from web servers, data base servers, key-value stores, and 
popular utility programs. Todo: performance highlights, recovery highlights.

\subsection{\msmr: A Pracitcal SMR sytem for general multithreaded programs} \label{sec:replication-msmr}
TBD.

\subsubsection{Reaching consensus on socket APIs} \label{sec:replication-msmr-consensus}
TBD.

\subsubsection{Read-only optimization} \label{sec:replication-msmr-readonly}
For speed, modern SMR systems introduce read-only optimization: for many applications such as web servers and key-value stores,
 most of read operations do not modify applcation states, and SMR systems process them rapidly in 
local replicas without reaching consensus for these operations.

Unfortunately, in the multithreading settings, some read-only requests such as http GET requests may
modify the application's internal cache, potentially causing divergence of both application states and 
schedules on processing these requests.

In order to support read-only optimizations in \msmr, we leverage the performance critical 
section (a.k.a pcs) in \parrot~cite{parrot:sosp13}, so that we could process 
these read-only operations rapidly without needing consensus. Todo: also 
mention the client side design for identifying read-only requests.

In order to address the above challenge that read-only reqeusts may modify internal 
application cache and causing schedule divergence, we leverage a key 
insight: although the first few of them may modify application's internal states, this 
sequence tend to make the application finally converge to a stable state. Based 
on this insight, we could design an approach to soundly detect the application 
state changes by checking the schedule hash serving current request, and 
safely roll back to a 'soft' checkpoint if we detect state changes and 
deliver these first few read requests to the primary. Our approach can soundly 
detect application state changes (including even application internal cache 
expiration) that may affect schedules. And this approach enables our system 
to be very fast: run even faster than the single node un-replicated 
application.

Although this read-only optimization approach is simple and sound, it has to 
address a few challenges. First, although the internal cache modified by read-only 
requests may affect the schedules of read or write requests. Second, the 
soundness of this approach should not be weakened by corner case events in the 
distributed systems.

In order to address the first challenge (\ie, read-only requests may affect 
other read or write requests), our key approach is: maintain schedule hash for write 
requests. We consider all these three cases:

(1) The same (input) read requests affecting themselves. This is avoided by 
having the schedule hash for read-only requests, as mentioned above.

(2) Read requests affecting write requests. For each ``accept" message of a 
write request sent by the \paxos primary node, we attach the schedule hash (for 
the first operation, it is empty, and this has no conflict) of 
the last executed operation in the this ``accept" message with the ``committed" 
view stamp, so that all the replicas can compare this schedule hash with its 
own ones after executing each request. This carefuly design does not require 
the primary to execute the current request first, get the schedule hash, and 
then start consenses, because we attach the previous schedule hash. Once a 
replica executes a write request, and founds that the current schedule hash is 
different from the ones sent from the primary, current replica just rolls back 
to a soft checkpoint and then replays only requests that require consenses (the 
replica can safely ignore read-only requests). We expect this roll back is 
rare, because the application internal cache tends to be a per-input property 
and should rarely affect other inputs.

(3) Read requests affecting other read requests. This can also be detected by 
the schedule hash for each read request, because schedule hash is the unique ``stable" 
signature of each read request, and once some read requests are affected by 
other read requests, their schedule hash becomes different.

To address the second challenges, we discuss our approach on three (Todo: more?) popular 
corner cases:

(1) Packet loss causing schedule hash to get lost. This won't occur, because 
for each node in \msmr, the consensus proxy and the application shares the same 
physical machine, and the schedule hash executed by the application is recorded 
persistently by us.

(2) Node crash. For this case, in the restart phase, we simply erase all 
schedule hash, because current in memory application state does not match the 
application memmory states implied in schedule hash any more. Note that 
schedule hash is just our hints, and it can be safely erased in any case 
without affecting soundness or correctness of our system.

(3) Network partition or \paxos message lost. In this case, we can do nothing 
special (but we can still serve requests, \eg, read-only requests), because 
later if the network gets back, our above schedule hash can detect all types of 
read/write conflicts and roll back; if the network never gets back, this node 
can not reach any consensus with other nodes and then won't execute any write 
requests.

%% One may ask, can we just maintain the schedule hash for the write requests 
%% only, and then just simply roll back when we see a divergence of the schedule 
%% hash? Yes, this is a correct design, but it may trigger lots of roll backs, 
%% because the first few read only requests may cause the logical clock  ??? Not 
%% really.


\subsection{Implementation} \label{sec:replication-impl}
TBD.

\subsubsection{Checkpointing and recovery} \label{sec:replication-impl-checkpoint}
TBD.

\subsubsection{Handling nondeterministic operations} \label{sec:replication-impl-nondet}
TBD.

\subsection{Evaluation plan} \label{sec:replication-eval}
Performance overhead with the single node multithreaded version.

Performance breakdown (analysis) of the consensus component and the read-only optimization component.

Race detection results.

Recovery scenarios and performance results.



