\section{Leveraging \smt to Build Transparent Multithreaded State Machine Replication} \label{sec:replication}

In this section, we first introduce the background of state machine replication 
and its current limitations on building state machine replications for general 
multithreaded programs, and then present our design.

\subsection{Background} \label{sec:replication-intro}

Due to the trends of cloud computing, building higly-
available and fast applications have become critical. In order to 
provide high availability, the state machine replication (or SMR) approach has become 
popular. Todo: briefly introduce SMR use Paxos to totally order all requests. In order to
build fast services, people introduce read-only optimizations for read requests.

Unfortunately, despite much effort from academia and industry, it is still 
insufficient for software vendors to provide practical state machine 
replication service to general applications, because of two challenges. Existing 
state machine replication techniques typically require developers to replicate 
a specific subset of program states (with APIs) instead of the whole program 
states, which requires strong distributed sytems expert knowledge, and error prone 
(cite the MongoDB exmample). Second, existing SMR infrastructures assume all 
replicas always run the same schedule for the same input, pracitally restricing 
the number of running threads to be only 1 and preventing them fully leverage the 
power of the multi-core hardware.

Addressing the first challenge requires a general design of APIs for SMR. 
Todo: motivate people that we use design the interface at the socket API level.

Addressing the second challenge requires a systematic way of to ensure the same 
schedules. Todo: introduce \smt, which can greatly reduce the number of schedules for 
multithreaded programs.

This article made two major contributions towards building practical state-
machine replications for multithreaded programs. First, how to reach 
consensus on general network inputs (including input values and timings) 
across replicas. Second, how to perform read-only 
optimization for general applications, which is hard, because some 
semantically read only operations (such as get() in key-value store and GET 
in http servers) may modify states of server programs. Todo: should we 
include any other challenges that we have discussed? E.g., recovery, 
leveraging StableMT, and logical clocks.

Our system is the first system on building practical state-machine 
replications for general multi-threaded programs. Todo: how it works: reach 
consensus on general socket operations, LD\_PRELOAD, transparent (do not need 
to annotate shared states in a program) replications for servers.

Introduce the fault-tolerance and performance features (guarantees) of our 
system.

Practical highlights of our system: evaluated a good range of popular server 
programs, ranging from web servers, data base servers, key-value stores, and 
popular utility programs. Todo: performance highlights, recovery highlights.

\subsection{\msmr: A Pracitcal SMR sytem for general multithreaded programs} \label{sec:replication-msmr}
TBD.

\subsubsection{Reaching consensus on socket APIs} \label{sec:replication-msmr-consensus}
TBD.

\subsubsection{Read-only optimization} \label{sec:replication-msmr-readonly}
For speed, modern SMR systems introduce read-only optimization: for many applications such as web servers and key-value stores,
 most of read operations do not modify applcation states, and SMR systems process them rapidly in 
local replicas without reaching consensus for these operations.

Unfortunately, in the multithreading settings, some read-only requests such as http GET requests may
modify the application's internal cache, potentially causing divergence of both application states and 
schedules on processing these requests.

In order to support read-only optimizations in \msmr, we leverage the performance critical 
section (a.k.a pcs) in \parrot~cite{parrot:sosp13}, so that we could process 
these read-only operations rapidly without needing consensus. Todo: also 
mention the client side design for identifying read-only requests.

In order to address the above challenge that read-only reqeusts may modify internal 
application cache and causing schedule divergence, we leverage a key 
insight: although the first few of them may modify application's internal states, this 
sequence tend to make the application finally converge to a stable state. Based 
on this insight, we could design an approach to soundly detect the application 
state changes by checking the schedule hash serving current request, and 
safely roll back to a 'soft' checkpoint if we detect state changes and 
deliver these first few read requests to the primary. Our approach can soundly 
detect application state changes (including even application internal cache 
expiration) that may affect schedules. And this approach enables our system 
to be very fast: run even faster than the single node un-replicated 
application.

Although this read-only optimization approach is simple and sound, it has to 
address a few challenges. First, although the internal cache modified by read-only 
requests may affect the schedules of write requests. Second, as part of the SMR 
system, our design has to reach the same fault tolerance level of \paxos.

The root cause of the first challenge can be equally (todo: it 
this true??? need a proof) redescribed in this way: write requests 
arrive before read-only requests push the application to converge to a stable 
state. Our approach is: whenever write request arrive before read requests 
have reached a stable state, we simply erase the hashes of all read requests 
in all replicas. (three examples: RRWWR..., RWRWRRR..., RRRRRR(cache timeout)WR.... For 
the first two examples, we can handle that, how about the last one???)

Todo: there can be two approaches on erashing hashes: first, erase all read hashes 
whenever a write request has come; second, erase all unstable read hashes 
whenever a write request has come.

In order to address the second challenges, we've considered these corner cases. 
Todo: add in Cheng's design after discussing with him. Are these corner cases 
complete?


\subsection{Implementation} \label{sec:replication-impl}
TBD.

\subsubsection{Checkpointing and recovery} \label{sec:replication-impl-checkpoint}
TBD.

\subsubsection{Handling nondeterministic operations} \label{sec:replication-impl-nondet}
TBD.

\subsection{Evaluation plan} \label{sec:replication-eval}
Performance overhead with the single node multithreaded version.

Performance breakdown (analysis) of the consensus component and the read-only optimization component.

Race detection results.

Recovery scenarios and performance results.



